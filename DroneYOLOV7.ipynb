{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZpGQmgIE4ru"
   },
   "source": [
    "# **1. Installing Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsaSUvj6FX98"
   },
   "source": [
    "## 1.1 Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQhvetA-L3Z-",
    "outputId": "c49444ab-60db-451e-8212-b806fd08a1e7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-coGSPIFkO6"
   },
   "source": [
    "## 1.2 Installing our dependencies\n",
    "\n",
    "**Note**: While installing dependencies it will prompt to restart runtime, don't worry just restart it and only run the above **1.1 Mounting google drive** cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PGQkL0hkMEC2",
    "outputId": "5bd9f696-2926-4616-aff9-da35d07767c0"
   },
   "outputs": [],
   "source": [
    "# Download YOLOv7 repository and install requirements\n",
    "\n",
    "%cd /content/gdrive/MyDrive\n",
    "!git clone https://github.com/augmentedstartups/yolov7.git\n",
    "%cd yolov7\n",
    "!pip install -r requirements.txt\n",
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNq_xvfwF65W"
   },
   "source": [
    "# **2. Getting Our Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzNG2JFDMQ5q",
    "outputId": "29d3c7e8-c315-41ab-d221-ba285d5adff3"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/yolov7\n",
    "\n",
    "#### ROBOFLOW DATASET DOWNLOAD CODE #####\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"xxxxxxxxxxxxx\")\n",
    "project = rf.workspace(\"vinitawale\").project(\"drones-uav-yolov5\")\n",
    "dataset = project.version(1).download(\"yolov7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYAA-B5fKLW6"
   },
   "source": [
    "# **3. Run YOLOv7 Training**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieCqTaadHMwe"
   },
   "source": [
    "## 3.1 Getting our pretrained model, you can choose any model from below to fine-tune\n",
    "\n",
    "**Uncomment the model you want to finetune**\n",
    "\n",
    "There are five available model, uncomment the one which you want to train. For this we will be finetuning **yolov7.pt** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnoD0kiwdcim",
    "outputId": "fb12838f-077c-439e-ff8c-12a86c9fdb24"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Mz-HigZM2xo",
    "outputId": "d34c05c7-e2e5-4125-f985-67b8dfbdec61"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
    "#wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt\n",
    "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt\n",
    "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt\n",
    "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt\n",
    "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm3t5iCBHq8s"
   },
   "source": [
    "## 3.2 Start Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ80gpjImvGE",
    "outputId": "989fe831-b283-4b29-d039-b054795df895"
   },
   "outputs": [],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\"\"\"\n",
    "import keras\n",
    "keras.__version__\n",
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "\n",
    "for val in local_device_protos:\n",
    "  print(val.device_type)\n",
    "\n",
    "import keras\n",
    "keras.backend.set_session(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssOC8XWZN25Y",
    "outputId": "ae5e5e27-7bd8-41d2-cf45-049e16809e5b"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/yolov7\n",
    "!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 10 --data {dataset.location}/data.yaml --weights 'yolov7.pt' --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdyN-la5LYVm"
   },
   "source": [
    "# **4. Evaluation**\n",
    "\n",
    "- Note the checkpoints from training will be stored by default in runs/train/exp. Take the path of the latest checkpoint\n",
    "\n",
    "We can evaluate the performance of our custom training using the provided evalution script.\n",
    "\n",
    "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_iqHZtn94zc"
   },
   "source": [
    "## 4.1 F1 and Precision Recall Curve & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TYjFikI48Ngf",
    "outputId": "be7f7ea0-3f4b-41db-df09-119be40d8e6b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp5/F1_curve.png\", width=1000, height=600))\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp5/PR_curve.png\", width=1000, height=600))\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp5/confusion_matrix.png\", width=1000, height=600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM3ycj-Wy5Yh"
   },
   "source": [
    "## 4.2 Precision and Recall Confidence Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-ZKIbyObyZOb",
    "outputId": "41cd05a3-467b-4837-dcd0-bb0769353ee4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp5/P_curve.png\", width=1000, height=600))\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp5/R_curve.png\", width=1000, height=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YifXIV3YzDwM"
   },
   "source": [
    "# **5. Evaluation on Test Images**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pYQASsRNEKX"
   },
   "source": [
    "## 5.1.1 Run the below cell to evaluate on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4subGsgFOKXq"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "!python detect.py --weights /content/gdrive/MyDrive/yolov7/runs/train/exp5/weights/epoch_009.pt --conf 0.1 --source /content/gdrive/MyDrive/yolov7/Drones-UAV-YoloV5-1/test/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af7q-cEONIYI"
   },
   "source": [
    "## 5.1.2 Display Inference on Folder of Test Images\n",
    "\n",
    "**Note** From the above output display copy the full path of folder where test images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "79W_rpa1MGMp",
    "outputId": "a8186068-bf44-4f99-db9e-1e08952aa1be"
   },
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "i = 0\n",
    "limit = 10000 # max images to print\n",
    "for imageName in glob.glob('/content/gdrive/MyDrive/yolov7/runs/detect/exp/*.jpg'):\n",
    "    #Assuming JPG\n",
    "    if i < limit:\n",
    "      display(Image(filename=imageName))\n",
    "      print(\"\\n\")\n",
    "    i = i + 1\n",
    "\n",
    "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/detect/exp/318_JPEG.rf.310f31932fdbd01365790ebca3182766.jpg\", width=400, height=400))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2EWkmufOChU"
   },
   "source": [
    "## **5.2 Now it's time to Infer on Custom Images**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbp1q19cOrER"
   },
   "source": [
    "## 5.2.1 Helper Code For Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tkfW_FBM4IW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive/yolov7')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "\n",
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iis5R9nO6kf"
   },
   "source": [
    "## 5.2.2 Configuration Parameters\n",
    "\n",
    "Change the path of both **weights** and **yaml** file\n",
    "\n",
    "**weights** will be in yolov7 main folder -> runs -> train and then select the appropriate weight\n",
    "\n",
    "**yaml** yolov7 main folder -> Drone, there you will find yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR69IOOpNnb4"
   },
   "outputs": [],
   "source": [
    "classes_to_filter = None  #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
    "\n",
    "\n",
    "opt  = {\n",
    "\n",
    "    \"weights\": \"/content/gdrive/MyDrive/yolov7/runs/train/exp5/weights/best.pt\", # Path to weights file default weights are for nano model\n",
    "    \"yaml\"   : \"yolov7/Drones-UAV-YoloV5-1/data.yaml\",\n",
    "    \"img-size\": 640, # default image size\n",
    "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
    "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
    "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
    "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDiFruFSPsYQ"
   },
   "source": [
    "## **5.3. Inference on Single Image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnKI_cii2xXY",
    "outputId": "b92ac98a-db33-402c-b930-57c53388ede6"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/yolov7\n",
    "!gdown https://drive.google.com/uc?id=1c96hId8WNsOASKHcAxsQeM4N-N2wuwy9\n",
    "#This does not work in Safari Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReP3h5j-26rw"
   },
   "outputs": [],
   "source": [
    "source_image_path = '/content/gdrive/MyDrive/yolov7/Drones-UAV-YoloV5-1/valid/images/113_JPEG.rf.480bf028f9d728bbadcc09a5d8ebbc44.jpg'\n",
    "#Change the Path Name to your file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPtGIYnSPnvj",
    "outputId": "70d372be-5c18-4f81-81d4-130696da10e7"
   },
   "outputs": [],
   "source": [
    "# Give path of source image.\n",
    "#%cd /content/gdrive/MyDrive/yolov7\n",
    "#source_image_path = '/content/trash.png'\n",
    "\n",
    "with torch.no_grad():\n",
    "  weights, imgsz = opt['weights'], opt['img-size']\n",
    "  set_logging()\n",
    "  device = select_device(opt['device'])\n",
    "  half = device.type != 'cpu'\n",
    "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "  stride = int(model.stride.max())  # model stride\n",
    "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "  if half:\n",
    "    model.half()\n",
    "\n",
    "  names = model.module.names if hasattr(model, 'module') else model.names\n",
    "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "  if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "  img0 = cv2.imread(source_image_path)\n",
    "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "  img = np.ascontiguousarray(img)\n",
    "  img = torch.from_numpy(img).to(device)\n",
    "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "  if img.ndimension() == 3:\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "  # Inference\n",
    "  t1 = time_synchronized()\n",
    "  pred = model(img, augment= False)[0]\n",
    "\n",
    "  # Apply NMS\n",
    "  classes = None\n",
    "  if opt['classes']:\n",
    "    classes = []\n",
    "    for class_name in opt['classes']:\n",
    "\n",
    "      classes.append(opt['classes'].index(class_name))\n",
    "\n",
    "\n",
    "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
    "  t2 = time_synchronized()\n",
    "  for i, det in enumerate(pred):\n",
    "    s = ''\n",
    "    s += '%gx%g ' % img.shape[2:]  # print string\n",
    "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
    "    if len(det):\n",
    "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "      for c in det[:, -1].unique():\n",
    "        n = (det[:, -1] == c).sum()  # detections per class\n",
    "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "      for *xyxy, conf, cls in reversed(det):\n",
    "\n",
    "        label = f'{names[int(cls)]} {conf:.2f}'\n",
    "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "pkBIq12rQPsY",
    "outputId": "1c9b94a0-6582-49ec-d973-8ec6cb7f5ff1"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "cv2_imshow(img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6jTZk9DRJtH"
   },
   "source": [
    "# **6. Inference on Video**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDitGHQ3SV3e"
   },
   "source": [
    "## 6.1 Enter Video Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYrG-9imSVMJ"
   },
   "outputs": [],
   "source": [
    "#give the full path to video, your video will be in the Yolov7 folder\n",
    "video_path = '/content/gdrive/MyDrive/drone_dataset/droneVid.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYRI2XkOSr7A"
   },
   "source": [
    "## 6.2 YOLOv7 Inference on Video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Njns91pRStLZ",
    "outputId": "51b9d4a3-2cf5-40a0-cec4-8c37decfe195"
   },
   "outputs": [],
   "source": [
    "# Initializing video object\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "#Video information\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialzing object for writing video output\n",
    "output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n",
    "torch.cuda.empty_cache()\n",
    "# Initializing model and setting it for inference\n",
    "with torch.no_grad():\n",
    "  weights, imgsz = opt['weights'], opt['img-size']\n",
    "  set_logging()\n",
    "  device = select_device(opt['device'])\n",
    "  half = device.type != 'cpu'\n",
    "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "  stride = int(model.stride.max())  # model stride\n",
    "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "  if half:\n",
    "    model.half()\n",
    "\n",
    "  names = model.module.names if hasattr(model, 'module') else model.names\n",
    "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "  if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "  classes = None\n",
    "  if opt['classes']:\n",
    "    classes = []\n",
    "    for class_name in opt['classes']:\n",
    "      classes.append(opt['classes'].index(class_name))\n",
    "\n",
    "  for j in range(nframes):\n",
    "\n",
    "      ret, img0 = video.read()\n",
    "      if ret:\n",
    "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "          img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment= False)[0]\n",
    "\n",
    "\n",
    "        pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
    "        t2 = time_synchronized()\n",
    "        for i, det in enumerate(pred):\n",
    "          s = ''\n",
    "          s += '%gx%g ' % img.shape[2:]  # print string\n",
    "          gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
    "          if len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "            for c in det[:, -1].unique():\n",
    "              n = (det[:, -1] == c).sum()  # detections per class\n",
    "              s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "\n",
    "              label = f'{names[int(cls)]} {conf:.2f}'\n",
    "              plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "        print(f\"{j+1}/{nframes} frames processed\")\n",
    "        output.write(img0)\n",
    "      else:\n",
    "        break\n",
    "\n",
    "\n",
    "output.release()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "sMw_X57gSyM4",
    "outputId": "b42a09ad-749f-418e-9373-1879cd57e75e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import os\n",
    "\n",
    "# Input video path\n",
    "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
    "\n",
    "# Compressed video path\n",
    "compressed_path = \"/content/result_compressed.mp4\"\n",
    "\n",
    "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
    "\n",
    "# Show video\n",
    "mp4 = open(compressed_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQJPLimMS29q"
   },
   "source": [
    "## 6.3 Download Inference Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ-jbXXqS0F3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
    "files.download(save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
